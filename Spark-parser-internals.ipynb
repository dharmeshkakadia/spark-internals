{"nbformat_minor": 0, "cells": [{"source": "# Spark Parser\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### Catalyst Architecture", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "![Spark Architecture](https://raw.githubusercontent.com/dharmeshkakadia/dharmeshkakadia.github.io/master/images/spark-arch.png) ", "cell_type": "markdown", "metadata": {"collapsed": true, "slideshow": {"slide_type": "fragment"}}}, {"source": "### Need for a formally defined grammer and parser\n\nGrammar+Parser helps in answering the following question: Is given statement complaint to the rules of the language. \nSome ``select * from sample`` is valid Spark statement or not?\n\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### Why we need to understand parser \n\n* Will/Why is the given statement giving error on parsing?\n* Is this a keyword in Spark or not? \n* Add new feature (say merge statement support)\n* Build new tools, say our own editor for Spark..\n* Generating automated tests from the grammer", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "subslide"}}}, {"source": "### ANTLR\n\n* ANother Tool for Language Recognition. \n* Parser generator for reading, processing, executing, or translating structured text.\n* ANTLR generates a parser that can build and walk parse trees.\n* Toolkit for building languages\n* Used widely by many languages (Groovy, Cassandra, Hive, \u2026)\n* Spark parser first tries to parse it using SLL mode (Strong LL), which is faster. If that fails, it will try to parse it as LL.\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### ANTLR grammar \n\n* Spark grammar is LL. \n* LL stands for Left to right parsing, deriving leftmost derivation.\n\n\n* A grammar G = ( N, T, P, S ) is said to be strong LL(k) for some fixed natural number k if for all nonterminals A, and for any two distinct A-productions in the grammar. \n* The strong LL(k) grammars are a subset of the LL(k) grammars that can be parsed without knowledge of the left-context of the parse. That is, each parsing decision is based only on the next k tokens of the input for the current nonterminal that is being expanded. Or in other words, parsers that ignore the parser call stack for prediction are called Strong LL (SLL) parsers.\n\n\n* **Spark parser first tries to parse it using SLL mode (Strong LL), which is faster. If that fails, it will try to parse it as LL.**\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### ANTLR operators\n\n```\n| alternatives\n. any character\n? repeated zero or one time\n+ repeated one or more times\n* repeasted zero or more times\n```", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### Simple ANTLR end to end example\n\nA very simple codebase that implements the following [grammer](https://github.com/dharmeshkakadia/hello-antlr/blob/master/src/main/antlr4/Hello.g4):\n\n```antlr-java\ngrammar Hello;\nmsg   : 'Hello' ID;\nID  : [a-z]+ ;\nWS  : [ \\t\\r\\n]+ -> skip ;\n```\n\nThen we have to implement Visiter interface to decide what to do when we encounter/parse/traverse a node in the grammar\n\n```java\npublic class HelloVister extends HelloBaseListener {\npublic void enterMsg(HelloParser.MsgContext ctx){\n\tSystem.out.println(\"Entering Msg : \" + ctx.ID().getText());\n}\n\tpublic void exitMsg(HelloParser.MsgContext ctx){\n\t\tSystem.out.println(\"Exiting Msg\");\n\t}\n}\n```\n\nNow we have to initilize the lexer, parser etc.\n\n```java\nimport org.antlr.v4.runtime.ANTLRInputStream;\nimport org.antlr.v4.runtime.CommonTokenStream;\nimport org.antlr.v4.runtime.tree.ParseTreeWalker;\n\npublic class Hello {\n\tpublic static void main(String[] args) {\n\t\tHelloLexer lexer = new HelloLexer(new ANTLRInputStream(\"Hello world\"));\n\t\tHelloParser parser = new HelloParser(new CommonTokenStream(lexer));\n\t\tParseTreeWalker visiter = new ParseTreeWalker();\n\t\tvisiter.walk(new HelloVister(),parser.msg());\n\t}\n}\n\n```\n\n\nThis has a maven antlr integration, which takes care of generating the code for the grammar as well. To compile\n\n```\nmvn compile\n```\n\nTo run it, use\n\n```\nmvn exec:java -q\n```\n\nwhich would print\n```\nEntering Msg : world\nExiting Msg\n\n```", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### Spark Grammer\n\n* Spark Catalyst works over a specified grammar is defined by [SqlBase.g4](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/antlr4/org/apache/spark/sql/catalyst/parser/SqlBase.g4)\n\n* Spark uses AnTLR v4 parser generator.\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### Statement in Spark\n\n```antlr-java\nstatement\n    : query                                                            #statementDefault\n    | USE db=identifier                                                #use\n    | CREATE DATABASE (IF NOT EXISTS)? identifier\n        (COMMENT comment=STRING)? locationSpec?\n        (WITH DBPROPERTIES tablePropertyList)?                         #createDatabase\n    | ALTER DATABASE identifier SET DBPROPERTIES tablePropertyList     #setDatabaseProperties\n    | DROP DATABASE (IF EXISTS)? identifier (RESTRICT | CASCADE)?      #dropDatabase\n    | createTableHeader ('(' colTypeList ')')? tableProvider\n        (OPTIONS options=tablePropertyList)?\n        (PARTITIONED BY partitionColumnNames=identifierList)?\n        bucketSpec? locationSpec?\n        (COMMENT comment=STRING)?\n        (TBLPROPERTIES tableProps=tablePropertyList)?\n        (AS? query)?                                                   #createTable\n    | createTableHeader ('(' columns=colTypeList ')')?\n        (COMMENT comment=STRING)?\n        (PARTITIONED BY '(' partitionColumns=colTypeList ')')?\n        bucketSpec? skewSpec?\n        rowFormat?  createFileFormat? locationSpec?\n        (TBLPROPERTIES tablePropertyList)?\n        (AS? query)?                                                   #createHiveTable\n    | CREATE TABLE (IF NOT EXISTS)? target=tableIdentifier\n        LIKE source=tableIdentifier locationSpec?                      #createTableLike\n    | ANALYZE TABLE tableIdentifier partitionSpec? COMPUTE STATISTICS\n        (identifier | FOR COLUMNS identifierSeq)?                      #analyze\n    | ALTER TABLE tableIdentifier\n        ADD COLUMNS '(' columns=colTypeList ')'                        #addTableColumns\n    | ALTER (TABLE | VIEW) from=tableIdentifier\n        RENAME TO to=tableIdentifier                                   #renameTable\n    | ALTER (TABLE | VIEW) tableIdentifier\n        SET TBLPROPERTIES tablePropertyList                            #setTableProperties\n    | ALTER (TABLE | VIEW) tableIdentifier\n        UNSET TBLPROPERTIES (IF EXISTS)? tablePropertyList             #unsetTableProperties\n    | ALTER TABLE tableIdentifier partitionSpec?\n        CHANGE COLUMN? identifier colType colPosition?                 #changeColumn\n    | ALTER TABLE tableIdentifier (partitionSpec)?\n        SET SERDE STRING (WITH SERDEPROPERTIES tablePropertyList)?     #setTableSerDe\n    | ALTER TABLE tableIdentifier (partitionSpec)?\n        SET SERDEPROPERTIES tablePropertyList                          #setTableSerDe\n    | ALTER TABLE tableIdentifier ADD (IF NOT EXISTS)?\n        partitionSpecLocation+                                         #addTablePartition\n    | ALTER VIEW tableIdentifier ADD (IF NOT EXISTS)?\n        partitionSpec+                                                 #addTablePartition\n    | ALTER TABLE tableIdentifier\n        from=partitionSpec RENAME TO to=partitionSpec                  #renameTablePartition\n    | ALTER TABLE tableIdentifier\n        DROP (IF EXISTS)? partitionSpec (',' partitionSpec)* PURGE?    #dropTablePartitions\n    | ALTER VIEW tableIdentifier\n        DROP (IF EXISTS)? partitionSpec (',' partitionSpec)*           #dropTablePartitions\n    | ALTER TABLE tableIdentifier partitionSpec? SET locationSpec      #setTableLocation\n    | ALTER TABLE tableIdentifier RECOVER PARTITIONS                   #recoverPartitions\n    | DROP TABLE (IF EXISTS)? tableIdentifier PURGE?                   #dropTable\n    | DROP VIEW (IF EXISTS)? tableIdentifier                           #dropTable\n    | CREATE (OR REPLACE)? (GLOBAL? TEMPORARY)?\n        VIEW (IF NOT EXISTS)? tableIdentifier\n        identifierCommentList? (COMMENT STRING)?\n        (PARTITIONED ON identifierList)?\n        (TBLPROPERTIES tablePropertyList)? AS query                    #createView\n    | CREATE (OR REPLACE)? GLOBAL? TEMPORARY VIEW\n        tableIdentifier ('(' colTypeList ')')? tableProvider\n        (OPTIONS tablePropertyList)?                                   #createTempViewUsing\n    | ALTER VIEW tableIdentifier AS? query                             #alterViewQuery\n    | CREATE (OR REPLACE)? TEMPORARY? FUNCTION (IF NOT EXISTS)?\n        qualifiedName AS className=STRING\n        (USING resource (',' resource)*)?                              #createFunction\n    | DROP TEMPORARY? FUNCTION (IF EXISTS)? qualifiedName              #dropFunction\n    | EXPLAIN (LOGICAL | FORMATTED | EXTENDED | CODEGEN | COST)?\n        statement                                                      #explain\n    | SHOW TABLES ((FROM | IN) db=identifier)?\n        (LIKE? pattern=STRING)?                                        #showTables\n    | SHOW TABLE EXTENDED ((FROM | IN) db=identifier)?\n        LIKE pattern=STRING partitionSpec?                             #showTable\n    | SHOW DATABASES (LIKE pattern=STRING)?                            #showDatabases\n    | SHOW TBLPROPERTIES table=tableIdentifier\n        ('(' key=tablePropertyKey ')')?                                #showTblProperties\n    | SHOW COLUMNS (FROM | IN) tableIdentifier\n        ((FROM | IN) db=identifier)?                                   #showColumns\n    | SHOW PARTITIONS tableIdentifier partitionSpec?                   #showPartitions\n    | SHOW identifier? FUNCTIONS\n        (LIKE? (qualifiedName | pattern=STRING))?                      #showFunctions\n    | SHOW CREATE TABLE tableIdentifier                                #showCreateTable\n    | (DESC | DESCRIBE) FUNCTION EXTENDED? describeFuncName            #describeFunction\n    | (DESC | DESCRIBE) DATABASE EXTENDED? identifier                  #describeDatabase\n    | (DESC | DESCRIBE) TABLE? option=(EXTENDED | FORMATTED)?\n        tableIdentifier partitionSpec? describeColName?                #describeTable\n    | REFRESH TABLE tableIdentifier                                    #refreshTable\n    | REFRESH (STRING | .*?)                                           #refreshResource\n    | CACHE LAZY? TABLE tableIdentifier (AS? query)?                   #cacheTable\n    | UNCACHE TABLE (IF EXISTS)? tableIdentifier                       #uncacheTable\n    | CLEAR CACHE                                                      #clearCache\n    | LOAD DATA LOCAL? INPATH path=STRING OVERWRITE? INTO TABLE\n        tableIdentifier partitionSpec?                                 #loadData\n    | TRUNCATE TABLE tableIdentifier partitionSpec?                    #truncateTable\n    | MSCK REPAIR TABLE tableIdentifier                                #repairTable\n    | op=(ADD | LIST) identifier .*?                                   #manageResource\n    | SET ROLE .*?                                                     #failNativeCommand\n    | SET .*?                                                          #setConfiguration\n    | RESET                                                            #resetConfiguration\n    | unsupportedHiveNativeCommands .*?                                #failNativeCommand\n    ;\n\n\n\n```", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### Query definition in Spark\n\n```antlr-java\nquerySpecification\n    : (((SELECT kind=TRANSFORM '(' namedExpressionSeq ')'\n        | kind=MAP namedExpressionSeq\n        | kind=REDUCE namedExpressionSeq))\n       inRowFormat=rowFormat?\n       (RECORDWRITER recordWriter=STRING)?\n       USING script=STRING\n       (AS (identifierSeq | colTypeList | ('(' (identifierSeq | colTypeList) ')')))?\n       outRowFormat=rowFormat?\n       (RECORDREADER recordReader=STRING)?\n       fromClause?\n       (WHERE where=booleanExpression)?)\n    | ((kind=SELECT (hints+=hint)* setQuantifier? namedExpressionSeq fromClause?\n       | fromClause (kind=SELECT setQuantifier? namedExpressionSeq)?)\n       lateralView*\n       (WHERE where=booleanExpression)?\n       aggregation?\n       (HAVING having=booleanExpression)?\n       windows?)\n    ;\n\n```", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### Constants in Spark\n\n```antlr-java\nconstant\n    : NULL                                                                                     #nullLiteral\n    | interval                                                                                 #intervalLiteral\n    | identifier STRING                                                                        #typeConstructor\n    | number                                                                                   #numericLiteral\n    | booleanValue                                                                             #booleanLiteral\n    | STRING+                                                                                  #stringLiteral\n    ;\n\n```", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### Spark grammer identifiers\n\nSome samples.\n\nRelational Operators :\n```antlr-java\nEQ  : '=' | '==';\nNSEQ: '<=>';\nNEQ : '<>';\nNEQJ: '!=';\nLT  : '<';\nLTE : '<=' | '!>';\nGT  : '>';\nGTE : '>=' | '!<';\n\n```\n\nArithmatic operators:\n\n```antlr-java\nPLUS: '+';\nMINUS: '-';\nASTERISK: '*';\nSLASH: '/';\nPERCENT: '%';\nDIV: 'DIV';\nTILDE: '~';\nAMPERSAND: '&';\nPIPE: '|';\nCONCAT_PIPE: '||';\nHAT: '^';\n\n```\n\nString quotations:\n\n```antlr-java\nSTRING\n    : '\\'' ( ~('\\''|'\\\\') | ('\\\\' .) )* '\\''\n    | '\"' ( ~('\"'|'\\\\') | ('\\\\' .) )* '\"'\n    ;\n```\n\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "### Data types in Spark\n\n#### How data is represented in Spark\nAll the data types implements [Row API](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala), which represents one row of output from a relational operator. It allows both generic access by ordinal, which will incur boxing overhead for primitives, as well as native primitive access.\n\n\n* Primitive types\n    * String, Varchar, char\n    * Float, Integer, Short, Double, Long\n    * Byte\n    * Binary\n    * Boolean\n    * Timestamp, Date\n\n* Complex Types\n    * Array\n    * Struct\n    * Map\n    * Union\n    \n#### Formal definition of data types:\n\n```antlr-java\ndataType\n    : complex=ARRAY '<' dataType '>'                            #complexDataType\n    | complex=MAP '<' dataType ',' dataType '>'                 #complexDataType\n    | complex=STRUCT ('<' complexColTypeList? '>' | NEQ)        #complexDataType\n    | identifier ('(' INTEGER_VALUE (',' INTEGER_VALUE)* ')')?  #primitiveDataType\n    ;\n\n```\n\n#### Spark types to Java types\n\nHere is how Spark data types maps to Java types\n\n* BooleanType -> java.lang.Boolean\n* ByteType -> java.lang.Byte\n* ShortType -> java.lang.Short\n* IntegerType -> java.lang.Integer\n* FloatType -> java.lang.Float\n* DoubleType -> java.lang.Double\n* StringType -> String\n* DecimalType -> java.math.BigDecimal\n* DateType -> java.sql.Date\n* TimestampType -> java.sql.Timestamp\n* BinaryType -> byte array\n* ArrayType -> scala.collection.Seq (use getList for java.util.List)\n* MapType -> scala.collection.Map (use getJavaMap for java.util.Map)\n* StructType -> org.apache.spark.sql.Row\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"source": "## Hands on with Spark Parser\n\nLets look at creating expression trees manually and execute it. We will run select statement with a condition on a temporary table.\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"execution_count": 77, "cell_type": "code", "source": "%%info\n", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'name': u'remotesparkmagics'}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1510699372204_0025</td><td>spark</td><td>dead</td><td><a target=\"_blank\" href=\"http://hn0-spark3.bpfgnom1212ulfuspeftsf1sue.cx.internal.cloudapp.net:8088/proxy/application_1510699372204_0025/\">Link</a></td><td></td><td></td></tr><tr><td>2</td><td>application_1511309191625_0009</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn0-spark3.bpfgnom1212ulfuspeftsf1sue.cx.internal.cloudapp.net:8088/proxy/application_1511309191625_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.36:30060/node/containerlogs/container_e02_1511309191625_0009_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false, "slideshow": {"slide_type": "slide"}}}, {"execution_count": 78, "cell_type": "code", "source": "org.apache.spark.sql.SparkSession.setActiveSession(spark)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Original query\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"execution_count": 79, "cell_type": "code", "source": "List((1),(2),(3),(4),(5)).toDF(\"id\").createOrReplaceGlobalTempView(\"data\")\nsql(\"select * from global_temp.data where id < 4\").show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+---+\n| id|\n+---+\n|  1|\n|  2|\n|  3|\n+---+"}], "metadata": {"collapsed": false, "slideshow": {"slide_type": "fragment"}}}, {"source": "### Defining data\n\nLets define data and translate it into the internal representation spark uses for the data type.", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"execution_count": 80, "cell_type": "code", "source": "val data =List(1,2,3,4,5).map(value => org.apache.spark.sql.catalyst.InternalRow.apply(value))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "data: List[org.apache.spark.sql.catalyst.InternalRow] = List([1], [2], [3], [4], [5])"}], "metadata": {"collapsed": false, "slideshow": {"slide_type": "fragment"}}}, {"source": "### Defining schema\n\nSince we are creating the tree manually, we need to specify the datatypes, that normally would be resolved by catalog.", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"execution_count": 81, "cell_type": "code", "source": "val schema = org.apache.spark.sql.catalyst.expressions.AttributeReference(\"id\",org.apache.spark.sql.types.IntegerType)()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "schema: org.apache.spark.sql.catalyst.expressions.AttributeReference = id#75"}], "metadata": {"collapsed": false, "slideshow": {"slide_type": "fragment"}}}, {"source": "### Attaching schema to data\n\nNow that we have both data and schema, lets create a local relation. LocalRelation is a leaf logical plan that allow functions like collect or take to be executed locally, i.e. without using Spark executors.\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"execution_count": 82, "cell_type": "code", "source": "val localRelation = org.apache.spark.sql.catalyst.plans.logical.LocalRelation(Seq(schema),data)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "localRelation: org.apache.spark.sql.catalyst.plans.logical.LocalRelation =\nLocalRelation [id#75]"}], "metadata": {"collapsed": false, "slideshow": {"slide_type": "fragment"}}}, {"source": "### Defining filter expression\n\nNow we have to add the filter \"id < 4\" to our plan. Lets define the filter first. We will use LessThan expression with literal \"4\"\n", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"execution_count": 83, "cell_type": "code", "source": "val filterExpression = org.apache.spark.sql.catalyst.expressions.LessThan(schema,org.apache.spark.sql.catalyst.expressions.Literal(4))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "filterExpression: org.apache.spark.sql.catalyst.expressions.LessThan = (id#75 < 4)"}], "metadata": {"collapsed": false, "slideshow": {"slide_type": "fragment"}}}, {"source": "### Creating logical plan/expression tree\n\nNow lets create logical plan by applying the above filter to the local relation that we just created.", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"execution_count": 84, "cell_type": "code", "source": "val phyzicalPlans = spark.sessionState.planner.plan(logicalPlan)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "phyzicalPlans: Iterator[org.apache.spark.sql.execution.SparkPlan] = non-empty iterator"}], "metadata": {"collapsed": false, "slideshow": {"slide_type": "fragment"}}}, {"source": "### Executing the plan\n\nLet's execute the first plan that is returned by the planner. (For our simple logical plan, we actually have only one physical plan).", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "slide"}}}, {"execution_count": 85, "cell_type": "code", "source": "val results = phyzicalPlans.next.execute.collect", "outputs": [{"output_type": "stream", "name": "stdout", "text": "results: Array[org.apache.spark.sql.catalyst.InternalRow] = Array([0,1], [0,2], [0,3])"}], "metadata": {"collapsed": false, "slideshow": {"slide_type": "fragment"}}}, {"source": "we now have our results of 3 rows with values 1,2,3. You can confirm by printing them ", "cell_type": "markdown", "metadata": {"slideshow": {"slide_type": "fragment"}}}, {"execution_count": 86, "cell_type": "code", "source": " results.foreach(println)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[0,1]\n[0,2]\n[0,3]"}], "metadata": {"collapsed": false, "slideshow": {"slide_type": "fragment"}}}, {"source": "## Generating test queries automatically\n\nWe can use this ability to manipulate expression trees to generate test cases automatically. Here we will see how to generate different comparision operators for above schema and execute them.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 87, "cell_type": "code", "source": "val data =List(1,2,3,4,5).map(value => org.apache.spark.sql.catalyst.InternalRow.apply(value))\nval schema = org.apache.spark.sql.catalyst.expressions.AttributeReference(\"id\",org.apache.spark.sql.types.IntegerType)()\nval localRelation = org.apache.spark.sql.catalyst.plans.logical.LocalRelation(Seq(schema),data)\n\n\nfor (expression <- List(org.apache.spark.sql.catalyst.expressions.LessThan, org.apache.spark.sql.catalyst.expressions.LessThanOrEqual, org.apache.spark.sql.catalyst.expressions.GreaterThan, org.apache.spark.sql.catalyst.expressions.EqualTo)){\n    val filterExpression = expression(schema,org.apache.spark.sql.catalyst.expressions.Literal(4))\n    val logicalPlan = org.apache.spark.sql.catalyst.plans.logical.Filter(filterExpression,localRelation)\n\n    val phyzicalPlans = spark.sessionState.planner.plan(logicalPlan)\n    val results = phyzicalPlans.next.execute.collect\n    println(expression)\n    results.foreach(println)\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "LessThan\n[0,1]\n[0,2]\n[0,3]\nLessThanOrEqual\n[0,1]\n[0,2]\n[0,3]\n[0,4]\nGreaterThan\n[0,5]\nEqualTo\n[0,4]"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}, "celltoolbar": "Raw Cell Format"}}